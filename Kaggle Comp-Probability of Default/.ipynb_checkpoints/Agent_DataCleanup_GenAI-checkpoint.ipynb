{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b26d0ecf-1958-49cb-86ee-81854116ced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from transformers import pipeline\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1febf9fd-a417-42e8-b4f2-ccecdf6ffbc6",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c5e0bd4-7ec6-4339-adb8-0664dac7e0b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32581, 12)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"..\\\\Kaggle Comp-Probability of Default\\\\Training\\\\data\\\\credit_risk_dataset.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bfff38-2cb0-47ce-a428-2ebef10d8c7c",
   "metadata": {},
   "source": [
    "### Retrieve Hugging Face Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ead9708f-686f-4291-90f3-bad13c25fbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\pups_\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Log in to Hugging Face\n",
    "login(hf_token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c89344-88ca-4d1c-bcbc-328d6963a2e3",
   "metadata": {},
   "source": [
    "### Load Generative AI Model\n",
    "###### LLaMA3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4a46e36-2330-4bd0-982f-c1c88debc9d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'mistral'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#pipeline_falcon = pipeline(task=\"text-generation\", model= model_falcon, max_length=2048, temperature=0.5)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m model_mistral\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-Instruct-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m pipeline_mistral \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_mistral\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhf_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\genai-env\\lib\\site-packages\\transformers\\pipelines\\__init__.py:705\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    703\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 705\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[0;32m    708\u001b[0m custom_tasks \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\genai-env\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:940\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m--> 940\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    941\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    943\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[0;32m    944\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\genai-env\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:655\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_content[key]\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[1;32m--> 655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    656\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n\u001b[0;32m    657\u001b[0m module_name \u001b[38;5;241m=\u001b[39m model_type_to_module_name(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'mistral'"
     ]
    }
   ],
   "source": [
    "model_llama = \"meta-llama/Llama-3.1-70B\"\n",
    "#pipeline_llama = pipeline(task=\"text-generation\", model= model_llama,token=hf_token)\n",
    "#llm = HuggingFacePipeline(pipeline=llama_pipeline)\n",
    "\n",
    "model_falcon = \"tiiuae/falcon-7b-instruct\"\n",
    "#pipeline_falcon = pipeline(task=\"text-generation\", model= model_falcon, max_length=2048, temperature=0.5)\n",
    "\n",
    "model_mistral=\"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "pipeline_mistral = pipeline(task= \"text-generation\", model= model_mistral, use_auth_token= hf_token, device_map=\"auto\", trust_remote_code=True)\n",
    "#llm = HuggingFacePipeline(pipeline=pipeline_mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ff1506-7682-4b87-9884-07d23107efca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_prompt = \"\"\"\n",
    "You are an expert data scientist and a data cleanup agent.\n",
    "Given the following dataset summary, suggest the best steps for cleaning the data.\n",
    "\n",
    "Dataset summary:\n",
    "{data_summary}\n",
    "\n",
    "Respond with specific cleanup steps in Python code.\n",
    "\"\"\"\n",
    "\n",
    "# Prepare dataset summary\n",
    "data_summary = df.describe(include='all').to_string()\n",
    "print(data_summary)\n",
    "\n",
    "# Create the prompt template\n",
    "prompt = PromptTemplate(input_variables=[\"data_summary\"],template=cleanup_prompt)\n",
    "print(prompt)\n",
    "\n",
    "# Create an LLMChain to run the prompt\n",
    "cleanup_agent = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Get cleanup suggestions from LLaMA 3\n",
    "#cleanup_instructions = cleanup_agent.run(data_summary)\n",
    "#cleanup_instructions = cleanup_agent.invoke(data_summary)\n",
    "# print(\"LLaMA 3 Cleanup Instructions:\\n\")\n",
    "# print(cleanup_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e71b1b-c8d2-4368-a8be-5349bd748b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
